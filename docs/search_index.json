[["index.html", "S1Y Lab 5 Welcome to Lab 5 Intended Learning Outcomes Credit where credit is due", " S1Y Lab 5 Welcome to Lab 5 Intended Learning Outcomes This lab will investigate the ways in which the statistics from a random sample can serve to make inference and test hypotheses about the true parameter in the larger population. We will explore here inference methods for categorical data. Before we begin however, let's revisit the difference between categorical and numerical data. Categorical Data: Data where an observation can belong to one of a certain fixed number of categories or groups. For example, someone's bloodtype. Your bloodtype can be one of A, B, AB or O. Numerical Data: Data in the form of numbers; rather than being defined as a member of some group or category. It also must make sense for calculations to be performed on those numbers, such as adding them or taking averages of them. This is important, for example a phone number is a number but it doesn't make sense to add phone numbers together. An example of numerical data is your height. It's measured with a number you can perform calculations directly on such as averages. Another example would be a count of the number of patients seen at a GP Surgery in a day. The material within this lab has been based on OpenLearn Chapters 6 OpenIntro Page 206. Feel free to refer back to the materials to help you within this lab. Credit where credit is due The labs in S1Y/Z are a derivative of the labs on the OpenIntro Statistics website by the OpenIntro team used under a Creative Commons Attribution-ShareAlike 4.0 International License. Some of the artwork used is by @allison_horst "],["data.html", "Data", " Data In this Lab, we will explore and visualise the data using the tidyverse suite of packages and perform statistical inference. We will be looking at data from the Youth Risk Behaviour Surveillance System (YRBSS) survey, which uses data from American high school students to help discover health patterns. The dataframe is called yrbss and is part of the openintro package. We can access the dataframe and make a version with missing values removed using this code. library(openintro) yrbss &lt;- na.omit(yrbss) The dataframe yrbss contains 13 variables: • age: Age of the student. • gender: Gender of the student. • grade: School grade of the student. • hispanic: If the student is hispanic, or not. • race: Ethnicity of the student. • height: Height of the student, in metres. • weight: Weight of the student, in kilograms. • helmet_12m: How often the student wore a helmet while riding a bike in the last 12 months. • text_while_driving_30d: How many days out of the last 30 did the the student text while driving. • physically_active_7d: How many days out of the last 7 was the student physically active for at least an hour. • hours_tv_per_school_day: How many hours of TV does the student typically watch on a school night. • strength_training_7d: How many days out of the last 7 did the student lift weights. • school_night_hours_sleep: How many hours of sleep does the student typically get on a school night? First of all, it is always a good idea to look at the dataset you are working with to get a good sense of what it looks like, and the different types of data you may have, e.g. categorical or numerical. Run the code below to look at the first 6 rows of data and the structure of it. head(yrbss) str(yrbss) Recall from Lab 1, head() is a function where you put in it's brackets the name of the data frame, in this case yrbss, and it gives you the first 6 lines of that data to look at. You can press the arrows to go along and see all the variables. Again recall from Lab 1 str() is another function where you put the name of the data frame in the brackets, and this time it gives you the number of observations and the number of variables you have, the types of those variables e.g. \"int\" for an integer, \"num\" for a number and \"chr\" for a string of characters, as well as the first few values of each variable. How many observations/rows are there in the entire data set? Hint Look at the result of the str() function. "],["inference-for-categorical-data.html", "Inference for Categorical Data", " Inference for Categorical Data Figure 1: Blood type, an example of Categorical Data The code below is an example of presenting some sort of summary of the data frame yrbss. Specifically, the goal is to find out how many students there are in each of the different categories for how much sleep they typically get on a school night. You've seen both group_by() and summarize() functions in Lab 1 but it's worth having a refresher. First, you type the name of the overall dataframe you want to look at. In this case, yrbss. Then group the data by one of your variables using the group_by() function. The hours of sleep a night variable school_night_hours_sleep has been chosen but you could group by whichever variable you want based on the question of interest. Then what? You want some summary, so you use the summarize() function. You have to give your summary a name, in this instance we called it \"count\" but you could in theory call it whatever you like. Finally, since we want to find the counts of how many students get different amounts of sleep, we use the n() function which refers to the counts. Exercise 1 Adapt the code from the previous example to find out what the counts are within each category for the amount of days the students have texted while driving within the past 30 days. Look back to the data frame to find the name of the variable you want. Hint Use the group_by() and summarize() functions. The variable you want to group by is called text_while_driving_30d in yrbss. Solution yrbss %&gt;% group_by(text_while_driving_30d) %&gt;% summarize(count = n()) ## # A tibble: 8 x 2 ## text_while_driving_30d count ## &lt;chr&gt; &lt;int&gt; ## 1 0 3137 ## 2 1-2 658 ## 3 10-19 281 ## 4 20-29 224 ## 5 3-5 347 ## 6 30 583 ## 7 6-9 230 ## 8 did not drive 2891 Which category has the highest count? All 30 of the last 30 days. 0 of the last 30 days 6-9 of the last 30 days The students that didn't drive Hint Look for the highest count value from the grouped by summary. Exercise 2 Create a subset, called no_helmet, of the data frame yrbss using the code below. This subset no_helmet contains only the students that never wore a helmet, and a new variable text_every_day which is \"yes\" the student has texted and drove every day of the last 30 or \"no\" if they haven't. no_helmet &lt;- yrbss %&gt;% filter(helmet_12m == &quot;never&quot;) no_helmet &lt;- no_helmet %&gt;% mutate(text_every_day = ifelse(text_while_driving_30d == &quot;30&quot;, &quot;yes&quot;, &quot;no&quot;)) Again you've seen the filter() and mutate() functions in Lab 1. In the first 2 lines of the above code, we use the filter() function to subset yrbss and extract only some data, based on some rule. In this case we want only the students who never wore a helmet, i.e. the students for whom the helmet_12m variable equals \"never\". We stored this new subset of the data in the no_helmet object, but you could of course call it whatever you like. The &lt;- basically just tells R to store this new subset in an object called \"no_helmet\". In the second part of the code, we create the new variable text_every_day. no_helmet &lt;- no_helmet refers to updating the existing dataset \"no_helmet\" rather than creating an entirely new one. Then, (%&gt;%), we use the mutate() function to create a new variable based on some rule and add it to the dataset no_helmet. The rule here is to assign \"yes\" for every student whom texting while driving variable equals 30 and 0 otherwise. Find the proportion of students who have texted while driving every day in the past 30 days among those who never wore helmets. Hint We first want to group_by() the text_every_day variable just created, then count the number of students in each category and finally calculate the proportion of students for whom text_every_day is \"yes\" out of everyone who is the no_helmet dataset. Solution no_helmet %&gt;% group_by(text_every_day) %&gt;% summarize(count = n()) %&gt;% mutate(prop=count/sum(count)) ## # A tibble: 2 x 3 ## text_every_day count prop ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 no 4012 0.924 ## 2 yes 332 0.0764 What percentage of students have texted while driving every day in the last 30 days, and never wore a bike helmet? (to 2 decimal places) % "],["inference-on-proportions.html", "Inference on Proportions", " Inference on Proportions The Center for Disease Control and Prevention seeks to report the proportion of people who have texted while driving each day for the past 30 days. To do this, you can answer the question, \"What proportion of people in your sample reported that they have texted while driving each day for the past 30 days?\" with a statistic; while the question \"What proportion of people on earth have texted while driving each day for the past 30 days?\" is answered with an estimate of the parameter. The inferential tools for estimating population proportion are analogous to those used for means in the last Lab: the confidence interval and the hypothesis test. Exercise 3 Construct a confidence interval for the proportion of people who reported that they have texted while driving each day for the past 30 days. Recall from page 181 of OpenIntro that the 95% Confidence Interval for a proportion is given by \\[\\hat{p} \\pm 1.96*SE\\] where \\(SE = \\textrm{Standard Error} = \\sqrt\\frac{p(1-p)}{n}\\). The standard error is estimated by replacing \\(p\\) with its estimate \\(\\hat{p}\\). Here \\(n\\) is the sample size, 4344 in this case, and the critical value z equals 1.96 for the standard 95% confidence interval. As mentioned in the first lab, R can act like a calculator in the usual way, with * being the multiplying operator, / being the dividing operator and sqrt() being the square root operator. You have done this previously in Lab 4 to estimate the proportion of US adults who think climate change affects their local community. You can use your answer from the previous question to help you do this. Hint First calculate an estimate p for the proportion of people who have texted while driving each day (look at the output from Exercise 2 for the relevant numbers). The upper and lower bounds can then be calculated as p + 1.96*sqrt(p*(1-p)/n) and p - 1.96*sqrt(p*(1-p)/n). Solution p &lt;- 332/(332+4012) n &lt;- 332+4012 upper.bound &lt;- p + 1.96*sqrt(p*(1-p)/n) lower.bound &lt;- p - 1.96*sqrt(p*(1-p)/n) lower.bound ## [1] 0.06852646 upper.bound ## [1] 0.08432805 What could you interpret from this confidence interval? We are 90% confident the true proportion of non-helmet wearing students that texted while driving in all 30 of the previous 30 days is somewhere between 0.069 and 0.084. We are 95% confident the true proportion of non-helmet wearing students that texted while driving in all 30 of the previous 30 days is somewhere between 0.069 and 0.084. The proportion we calculated in outside of the interval calculated here, therefore we cannot make any valid interpretations. We are 95% confident the true proportion of non-helmet wearing students that texted while driving in all 30 of the previous 30 days is exactly 0.076, since this is inside the confidence interval. Hint Look in the textbook if you need a refresher on interpreting confidence intervals. Exercise 4 What is the margin of error for the estimate of the proportion of non-helmet wearers that have texted while driving each day for the past 30 days based on this survey? Referring back to the previous question, the margin of error is simply \\[ME = z*SE = z \\sqrt\\frac{p(1-p)}{n}\\] and for a standard 95% CI, \\[= 1.96 \\sqrt\\frac{p(1-p)}{n}\\] What is the margin or error for the proportion in question? (to 3 decimal places) Hint #calculate the margin of error in R using the code 1.96*sqrt(p*(1-p)/n) Exercise 5 Using the same method as seen above, repeat the process of constructing a confidence interval for the proportion of students who never wore a bike helmet and Had less than 5 hours sleep, then Strength trained every day, instead of texted while driving every day of the past 30 days. You should construct two confidence intervals, one for a) and one for b). You will need to answer the problem in two steps. Firstly, to construct a new \"yes\" or \"no\" variable, by adapting the below code and considering which variable to change and which category of that variable to change also i.e. what the == part equals. Secondly, you will need to construct a confidence interval in the same way as above, by working out your new \\(p\\) and \\(n\\) values using the same methods as before. Hint We have already created the dataset no_helmet and want to mutate it to add a new variable which indicates whether a student had less than 5 hours sleep (the number of hours slept is shown by the school_night_hours_sleep variable). Then, count the number of \"yes\"'s in this new variable and use this to calculate an estimate of the proportion p. Solution (a) no_helmet &lt;- no_helmet %&gt;% mutate(less5_hours_sleep = ifelse(school_night_hours_sleep == &quot;&lt;5&quot;, &quot;yes&quot;, &quot;no&quot;)) no_helmet %&gt;% group_by(less5_hours_sleep) %&gt;% summarize(count = n()) ## # A tibble: 2 x 2 ## less5_hours_sleep count ## &lt;chr&gt; &lt;int&gt; ## 1 no 4022 ## 2 yes 322 p &lt;- 322/(322+4022) n &lt;- 322+4022 upper.bound &lt;- p + 1.96*sqrt(p*(1-p)/n) lower.bound &lt;- p - 1.96*sqrt(p*(1-p)/n) lower.bound ## [1] 0.06633464 upper.bound ## [1] 0.08191582 Hint Within mutate, change the ifelse() statement to ifelse(physically_active_7d==7, \"yes\", \"no\") to create a new variable in no_helmet that takes the value \"yes\" if a student has strength trained every day for the past 7 days, and \"no\" otherwise. Solution (b) no_helmet &lt;- no_helmet %&gt;% mutate(train_every_day = ifelse(physically_active_7d==7,&quot;yes&quot;,&quot;no&quot;)) no_helmet %&gt;% group_by(train_every_day) %&gt;% summarize(count = n()) ## # A tibble: 2 x 2 ## train_every_day count ## &lt;chr&gt; &lt;int&gt; ## 1 no 3014 ## 2 yes 1330 p &lt;- 1330/(1330+3014) n &lt;- 1330+3014 upper.bound &lt;- p + 1.96*sqrt(p*(1-p)/n) lower.bound &lt;- p - 1.96*sqrt(p*(1-p)/n) lower.bound ## [1] 0.2924632 upper.bound ## [1] 0.3198757 How could you compare the two CI for proportions calculated above? We can make no comparisons between the two groups whatsoever at this stage of the analysis. The proportion of non helmet wearing strength trainers is probably much greater than the proportion of non-helmet wearing people with less than 5 hours sleep, since the CI is far above. The CI for the proportion of non-helmet wearing strength trainers seems far wider than the CI for the proportion of non-helmet wearing students with less than 5 hours sleep. The proportion of non-helmet wearing strength trainers probably equals the proportion of non-helmet wearing students that gets less than 5 hours sleep. Hint Look at where the first proportion's CI ends compared to where the second proportion's CI begins. "],["how-does-the-proportion-affect-the-margin-of-error.html", "How does the proportion affect the margin of error?", " How does the proportion affect the margin of error? Imagine you've set out to survey 1000 people on two questions: Are you at least 6-feet tall? Are you left-handed? Since both of these sample proportions were calculated from the same sample size, they should have the same margin of error, right? Wrong! While the margin of error does change with sample size, it is also affected by the proportion \\(p\\). Think back to the formula for the standard error, also seen on OpenIntro Page 172: \\(SE = \\sqrt{p(1-p)/n}\\). This is then used in the formula for the margin of error for a 95% confidence interval: \\[ ME = 1.96\\times SE = 1.96\\times\\sqrt{p(1-p)/n} \\,. \\] Since the population proportion \\(p\\) is in this \\(ME\\) formula, it should make sense that the margin of error is in some way dependent on the population proportion. We can visualise this relationship between \\(p\\) and \\(ME\\) by creating a plot of \\(ME\\) (on the y-axis) vs. \\(p\\) (on the x-axis). Since sample size is irrelevant to this discussion, let's just set it to some arbitrary value (\\(n = 1000\\)) and use this value in the following calculations: n &lt;- 1000 The first step is to create a variable p that can take any value between 0 and 1. We use the seq() function to create a sequence of possible values for \\(p\\) from 0 to 1 with each number incremented by 0.01. This is simply to create a set of values that we will eventually plot on the x-axis. You can then create a variable for the margin of error (ME) associated with each of these values of p using the familiar approximate formula (\\(ME = 2 \\times SE\\)). ` p &lt;- seq(from = 0, to = 1, by = 0.01) me &lt;- 2 * sqrt(p * (1 - p)/n) Lastly, you can plot the two variables, (the proportion p and the margin or error me), against each other to reveal their relationship. dd &lt;- data.frame(p = p, me = me) ggplot(data = dd, aes(x = p, y = me)) + geom_line() + labs(x = &quot;Population Proportion&quot;, y = &quot;Margin of Error&quot;) How would you describe the relationship between p and ME? They have an inverse cubic relationship. p and ME do not appear to be related. p and ME have a quadratic or parabolic relationship. They have a linear relationship. Hint What type of graph that you will have seen in Maths at school does this remind you of? Select the incorrect statement about the p/me equation/graph. Changing the value of n would make no difference to the graph. If we lower our confidence value, say from 95% to 90%, the range of plausible values for the margin of error would decrease. You can work out exactly for which value of p the margin of error is maximised mathematically using differentiation. If we were to increase the sample size n, the range of possible values for the margin of error would decrease. Hint You can play around with the equation/graph on something like Desmos online if you wish. Is this the relationship you expected based on the formula \\(ME = 1.96\\times\\sqrt{p(1-p)/n} \\,\\) ? For a given sample size, for which value of \\(p\\) is the margin of error (\\(ME\\)) maximized? ME is maximised when p = 0.33, or a third. ME is maximised when p = 1 ME is maximised when p = 0.5 ME is maximised when p = 0 Hint Look carefully at the value on the x axis that gives the largest value on the y axis. "],["success-failure-condition.html", "Success-failure condition", " Success-failure condition We have emphasised that you must always check conditions needed for central limit theorem to apply before making inference. For inference on proportions, the sample proportion can be assumed to be nearly normal if it is based on a random sample of independent observations and if both \\(np \\geq 10\\) and \\(n(1 - p) \\geq 10\\). This rule of thumb is easy enough to follow, but it makes you wonder: what's so special about the number 10? The short answer is: nothing! You could argue that you would be fine with 9 or that you really should be using 11. What is the \"best\" value for such a rule of thumb is, at least to some degree, arbitrary. However, when \\(np\\) and \\(n(1-p)\\) reaches 10 the sampling distribution is sufficiently normal to use confidence intervals and hypothesis tests that are based on that approximation. We can investigate the interplay between \\(n\\) and \\(p\\) and the shape of the sampling distribution by using simulations. We simulate the process of drawing a large number of samples of size \\(n\\) from a population with a true proportion of \\(p\\). For each of the samples we compute \\(\\hat{p}\\) and then plot a histogram to visualise their distribution. Using the numerical and graphical summaries below, describe the sampling distribution of sample proportions at \\(n=100\\) and \\(p=0.1\\). Be sure to note the centre, spread, and shape. ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.02339 0.08862 0.10223 0.10472 0.12438 0.18306 Describe the sampling distribution of sample proportions at n=100 and p=0.1. Consider the centre, spread, and shape. The distribution seems approximately Binomial, centred around the median and a tiny spread/standard deviation since n=100 is huge. The distribution seems approximately Normal, centred around the mean (approximately equal to p) and a relatively large spread/standard deviation since n=100 isn’t particularly large. The distribution seems approximately Poisson, centred around the mean (approximately equal to p) and a relatively large spread/standard deviation since n=100 isn’t particularly large. The distribution seems approximately Normal, centred around n=100 and a relatively large spread/standard deviation since n=100 isn’t particularly large. Hint Look carefully at the centre, shape and spread, comparing to the numerical summaries as well. Run the code below, changing the value of n to see how the sampling distribution of sample proportions varies as \\(n\\) varies. p &lt;- 0.15 n &lt;- 100 data1 &lt;- rnorm(n=n, mean=p, sd=sqrt(p*(1-p)/n)) ggplot(mapping = aes(x = data1)) + geom_histogram(color = &quot;white&quot;) + xlim(0, 0.3) + labs(y =&quot;Count&quot;, x = &quot;p-hats&quot;) How does \\(n\\) appear to affect the distribution of p-hat? There doesn't seem to be any significant effect of n on p-hat. An increase/decrease in n appears to shift the centre point of the distribution left/right. A decrease in n does not appear to affect the centre/symmetric point of the graph, however it does result in a much wider/more spread out distribution. An increase in n appears to give a wider, more spread out distribution of p-hat. Hint Look carefully at the rough centre/symmetric points of the graphs, and consider the width/spread of the overall graphs. Run the code below, changing the value of p to see how the sampling distribution of sample proportions varies as \\(p\\) varies. p &lt;- 0.1 n &lt;- 100 data2 &lt;- rnorm(n=n, mean=p, sd=sqrt(p*(1-p)/n)) ggplot(mapping = aes(x = data2)) + geom_histogram(color = &quot;white&quot;) + xlim(0, 1) + labs(y =&quot;Count&quot;, x = &quot;p-hats&quot;) How does \\(p\\) appear to affect the distribution of p-hat? There doesn't seem to be any significant effect of p on p-hat. A decrease in p does not appear to affect the width, however the centre/symmetric point shifts. Increasing p shifts the centre/symmetric point of the distribution to the left. The width/spread of the distribution also decreases as p decreases. Hint Look carefully at the rough centre/symmetric points of the graphs, and consider the width/spread of the overall graph. "],["group-tasks.html", "Group Tasks", " Group Tasks In August of 2012, news outlets ranging from the Washington Post to the Huffington Post ran a story about the rise of atheism in America. The source for the story was a poll that asked people, “Irrespective of whether you attend a place of worship or not, would you say you are a religious person, not a religious person or a convinced atheist?” This type of question, which asks people to classify themselves in one way or another, is common in polling and generates categorical data. Load the atheism data set into your workspace by typing the following code in an Rscript file in your local Rstudio. Feel free to explore the data before completing the following tasks. download.file(&quot;http://www.openintro.org/stat/data/atheism.RData&quot;, destfile = &quot;atheism.RData&quot;) load(&quot;atheism.RData&quot;) How many countries are included with the atheism data set? (Give your answer as a whole number) Task 1 Create a 95% confidence interval for the proportion of Spanish people identify as \"atheist\" in 2005. You will first need to filter the atheism data set by nationality being \"Spain\", and then by the year 2005. Task 2 Create a 95% confidence interval for the proportion of Spanish people identify as \"atheist\" in 2012 using the same method as before. Is there convincing evidence that Spain has seen a change in its atheism index between 2005 and 2012? The two confidence intervals overlap, therefore there is evidence of Spain seeing a change in it's atheism index The two confidence intervals do not overlap, therefore there is no evidence of Spain seeing a change in it's atheism index The two confidence intervals do not overlap, therefore there is evidence of Spain seeing a change in it's atheism index The two confidence intervals overlap, therefore there is no evidence of Spain seeing a change in it's atheism index Task 3 Repeat Tasks 1 and 2 to calculate two 95% confidence intervals for the proportion of Americans who identify as \"atheist\" in 2005 and in 2012. Is there convincing evidence that the USA has seen a change in its atheism index between 2005 and 2012? The two confidence intervals overlap, therefore there is no evidence of the USA seeing a change in it's atheism index The two confidence intervals overlap, therefore there is evidence of the USA seeing a change in it's atheism index The two confidence intervals do not overlap, therefore there is no evidence of the USA seeing a change in it's atheism index The two confidence intervals do not overlap, therefore there is evidence of the USA seeing a change in it's atheism index If in reality there has been no change in the atheism index in any of the countries, in how many of the countries would you expect to detect a change (at a significance level of 0.05) simply by chance? (to the nearest whole number) Hint We need to take the probability of a Type 1 error (rejecting the null hypothesis when \\(H_0\\) is actually true) and times it by 57 (the number of countries within atheism). The probability of a Type 1 error is simply the significance level alpha, in this case, 0.05. Task 4 Suppose you’re hired by the local government to estimate the proportion of residents that attend a religious service on a weekly basis. According to the guidelines, the estimate must have a margin of error no greater than 1% with 95% confidence. You have no idea what to expect for \\(p\\), so assume \\(p=0.5\\). Use R as a calculator to determine how many people would you have to sample to ensure that you are within the guidelines? Note You don't need to use the atheism data set to answer this question. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
